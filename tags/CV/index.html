<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="robots" content="noindex"><meta><title>标签: CV - Hexo</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Hexo"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Hexo"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="Hexo"><meta property="og:url" content="http://example.com/"><meta property="og:site_name" content="Hexo"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://example.com/img/og_image.png"><meta property="article:author" content="John Doe"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com"},"headline":"Hexo","image":["http://example.com/img/og_image.png"],"author":{"@type":"Person","name":"John Doe"},"publisher":{"@type":"Organization","name":"Hexo","logo":{"@type":"ImageObject","url":"http://example.com/img/logo.svg"}},"description":""}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 6.1.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="Hexo" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/tags">标签</a></li><li class="is-active"><a href="#" aria-current="page">CV</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-04-07T16:00:00.000Z" title="4/8/2020, 12:00:00 AM">2020-04-08</time>发表</span><span class="level-item"><time dateTime="2020-07-30T18:43:48.000Z" title="7/31/2020, 2:43:48 AM">2020-07-31</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E6%9C%AF%E4%B8%9A/">术业</a></span><span class="level-item">26 分钟读完 (大约3861个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/04/08/Major/04.Pixor/">基于鸟瞰图的点云目标检测：PIXOR</a></h1><div class="content"><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>该文章解决了在自动驾驶环境下从点云实时检测三维物体的问题。因为检测是安全的必要组成部分，所以计算速度至关重要。然而，由于点云的高维性，现有方法的计算成本很高。本文通过从鸟瞰图( BEV )中表示场景来更有效地利用3D数据，并提出PIXOR，这是一种无需处理的、单级的检测器，输出从像素级别的神经网络定向解码估计3D对象。模型的输入形式、网络架构和优化器是为了平衡高精度和实时效率而特别设计的。作者在两个数据集上验证PIXOR的效果——KITTI BEV对象检测基准数据集和大规模3D车辆检测基准数据集。在这两个数据集上，我们表明检测器在平均精度(AP)方面明显优于其他最先进的方法，同时仍以大于28 FPS的速度运行。</p>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>在过去的几年里，我们已经看到了大量利用卷积神经网络来产生精确的2D物体检测的方法，通常是从单个图像为主的模型（如Faster R-CNN和YOLO）。然而，在机器应用中，例如自主驾驶，我们对检测3D空间中的物体更为感兴趣。为了规划安全路线，这是运动规划的基础。</p>
<p>3D对象检测的最新方法利用不同的数据源。基于相机的方法利用单目或立体图像（双目摄像机RGB+HHA）。然而，从2D图像中精确的3D估计是困难的，特别是在长距离范围内。随着廉价RGB-D传感器如微软Kinect、英特尔RealSense和苹果PrimeSense的普及，出现了几种利用深度信息并将其与RGB图像融合的方法。与单目方法相比，它们已经显示出显著的性能提升。在自动驾驶的情况下，像LIDAR(光探测和测距)这样的高端传感器更常见，因为为了安全需要更高的精确度。处理LIDAR数据的主要困难是传感器以点云的形式产生非结构化数据，每360度扫描通常包含大约10^5个3D点。这给现代探测器带来了巨大的计算挑战。</p>
<p>在三维物体检测的背景下，已经探索了不同形式的点云表示。主要的想法是形成一个结构化的表示，其中可以应用标准卷积运算。现有表示主要分为两种类型: 3D体素网格和2D投影。3D体素网格将点云转换成规则间隔的3D网格，其中每个体素单元可以包含标量值(例如，占用率)或矢量数据(例如，根据该体素单元内的点计算的统计数据)。3D卷积通常用于从体素网格中提取高阶表示。然而，由于点云本质上是稀疏的，因此体素网格非常稀疏，很大一部分计算是冗余和不必要的，用这种表示的典型系统仅运行在1-2 FPS。</p>
<p>另一种方法是将点云投影到平面上，然后将平面离散为基于2D图像的表示，在此应用2D卷积。在离散化期间，手工制作的特征(或统计)被当做2D图像的像素值计算。常用的投影是范围视图(即360度全景视图)和鸟瞰图(即俯视图)。这些基于2D投影的表示更加紧凑，但是它们在投影和离散化过程中会带来信息损失。例如，范围视图投影将具有失真的对象大小和形状。为了减轻信息损失，MV3D建议将2D投影与摄像机图像融合，以带来额外信息。然而，融合模型相对于输入模态的数量具有近似线性的计算成本，使得实时应用不可行。</p>
<p>在这篇论文中，作者提出了一种精确的实时三维物体检测器，称之为PIXOR (ORiented 3D object detection from PIXel-wise neural network predictions)，它是在点云上运行的网络。PIXOR是一种单级、无二次处理（例如YOLO和SSD）的密集物体检测器，它以高效的方式利用2D鸟瞰图(BEV)表示。我们选择BEV表示，因为它在计算上比3D体素网格更友好，并且还保留了度量空间，这使得我们的模型能够探索关于对象类别大小和形状的先验。我们的探测器在鸟瞰图中输出真实世界尺寸的精确定向边界框。请注意，这些是三维估计，但必须假设物体在地面上，因为车辆不会飞，这也是自动驾驶场景中的合理假设。</p>
<p>作者在两个数据集上展示了其方法的有效性。具体来说，PIXOR在所有先前的方法中，实现了KITTI鸟瞰对象检测基准的最高平均精度(AP)，同时也是其中运行最快的(超过28FPS)。作者还在KITTI上提供了深入的消融研究，以调查每个模块贡献了多少性能增益，并通过将其应用于大规模TOR4D数据集来证明PIXOR的可扩展性和泛化能力。</p>
<h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>在本文中，作者提出了一种高效的三维物体检测器，它能够在给定LIDAR点云的情况下产生非常精确的边界框。这里的边界框估计不仅包含3D空间中的位置，还包含航向角，因为准确预测这对于自主驾驶非常重要。我们利用LIDAR点云的2D表示，因为它比3D体素网格表示更紧凑，因此可以进行实时推断。图1显示了提议的3D物体检测器的概述。</p>
<p><img src="http://ww1.sinaimg.cn/large/006E0Yd9gy1gh8muz5up6j30zb0cn0vm.jpg" alt="模型检测管道"></p>
<h3 id="Input-Representation"><a href="#Input-Representation" class="headerlink" title="Input Representation"></a>Input Representation</h3><p>由于要用标准卷积神经网络执行卷积操作，因此我们必须假设输入位于网格上。然而，3D点云是非结构化的，因此标准卷积不能直接应用在其上。一种选择是使用体素化来形成3D体素网格，其中每个体素单元包含位于该体素内的点的某些统计数据。为了从三维体素网格中提取特征表示，经常使用三维卷积。但这3DCNN的计算代价非常昂贵，并且因为LIDAR点云非常稀疏，以至于大多数体素单元都是空的。</p>
<p>相反，我们可以单独从鸟瞰图(BEV)中描绘场景。通过将维度从三维降低到二维，而不会丢失点云中的信息。因为我们可以将高度信息保留到颜色通道中。不仅如此，我们还有效地得到了更紧凑的表示方式，因为可以对BEV表示应用2D卷积。在自动驾驶的情况下，因为感兴趣的对象是在同一场地上，这种尺寸减小是合理的。除了计算效率之外，BEV表示还有其他优势。与前视图表示相比，由于对象不相互重叠，因此它简化了对象检测的问题。它还保留了度量空间，因此网络可以利用物体物理尺寸的先验信息。</p>
<p>体素化LIDAR表示的常用特征是占有率occupancy、强度intensity(也称反射率)、密度和高度特征。在PIXOR中，作者只使用占用率和强度作为特征。作者定义感兴趣的场景的三维物理尺寸L × W × H。然后，计算分辨率为dL × dW × dH的占用率特征，再计算分辨率为dL×dW×H的强度特征。注意，作者在占用率特征中增加了两个额外的通道，以覆盖范围外的点。最终表示的形状为L&#x2F;dL × W&#x2F;dW × (H&#x2F;dH + 3) </p>
<h3 id="Network-Architecture"><a href="#Network-Architecture" class="headerlink" title="Network Architecture"></a>Network Architecture</h3><p>PIXOR使用一个全卷积结构，设计用于密集定向的3D物体检测，并且没有使用类似于R-CNN的分支处理，相反网络在单个阶段输出像素预测，每个预测对应于3D对象估计。因此，根据定义，PIXOR的召回率是100%。多亏了完全非传统的架构，这样密集的预测可以非常有效地计算出来。就网络预测中3D对象的编码而言，作者构建了直接编码的模型，而不求助于预定义的object anchors。所有这些设计使得PIXOR变得非常简单，并且由于网络架构中的零超参数而得到很好的推广。具体来说，不需要设计object anchors，也不需要调整从第一阶段传递到第二阶段的关注区域以及相应的非最大抑制阈值。</p>
<p>整个体系结构可以分为两个子网络:主干网络Backbone network和头网络Header network。主干网络用于提取卷积特征并映射成输入的一般表示。它具有很高的表示能力来学习一个健壮的特征表示。头网络用于进行特定于任务的预测，在该例子中，它有一个具有多任务输出的单分支结构：对象分类和定位。<br><img src="http://ww1.sinaimg.cn/large/006E0Yd9gy1gh8muzjz2bj30h40dy0ti.jpg" alt="模型结构"></p>
<h3 id="Backbone-Network"><a href="#Backbone-Network" class="headerlink" title="Backbone Network"></a>Backbone Network</h3><p>卷积神经网络通常由卷积层和池化层组成。卷积图层用于提取输入要素的过度完整表示，而池化图层用于对要素地图大小进行下采样，以节省计算并帮助创建更健壮的表示。许多基于图像的物体检测器中的主干网络通常具有16倍下采样因子，并且通常被设计成具有更少的高分辨率层和更多的低分辨率层。它对图像很有效，因为对象通常像素尺寸很大。然而，在本文的情况下，这将导致一个问题，因为对象可能非常小。当离散分辨率为0.1m时，典型车辆的尺寸为18×40像素。16倍下采样后，它仅覆盖约3像素。</p>
<p>一个直接的解决方案是使用更少的池化层。然而，这将减小最终特征图中每个像素的感受野的大小，这限制了表现能力。另一个解决方案是使用膨胀卷积，但这将导致在高级特征地图中出现棋盘状伪像（checkerboard artifacts）。作者的解决方案很简单，使用16倍下采样因子，但是做了两个修改。首先，在较低的级别添加更多的通道数量较少的层，以提取更多的细节信息。其次，我们采用了类似FPN的自上而下的分支，将高分辨率特征图和低分辨率特征图相结合，以便对最终的特征表示进行上采样。</p>
<p>从Figure2可以看出主干网络中总共有五个层块。第一块由两个卷积层组成，通道为32，步长为1。第二至第五块由残差层组成（层数分别等于3、6、6、3）。每个残差块的第一卷积具有步长2，以便对特征图进行下采样。总的来说，下采样系数是16。为了对要素图进行上采样，其添加了一条自上而下的路径，每次对要素图进行2倍的上采样。然后，通过像素求和，这与相应分辨率的自下而上的特征图相结合。使用两个上采样层，这导致最终的特征图具有四倍下采样的大小。</p>
<h3 id="Header-Network"><a href="#Header-Network" class="headerlink" title="Header Network"></a>Header Network</h3><p>头网络是一个多任务网络——对象识别和方向定位。它被设计成小巧高效。分类分支输出1个通道特征图，跟sigmoid函数激活；回归分支输出线性的6通道特征图。在这两个分支之间分享权重的层数上存在权衡。一方面，我们希望更有效地利用重量。另一方面，由于它们是不同的子任务，我们希望它们更加独立和专业化。在下一章中，我们对这种权衡进行了一项调查性实验，发现两项任务的权重分配会带来稍微更好的性能。</p>
<p>我们将每个对象参数化为定向的bounding box b {θ，xc，yc，w，l}&#x3D;，每个元素对应于在[−π，π][−π，π]范围内的航向角、对象的中心位置(xc，yc)和对象的大小(w，l)。与基于长方体的三维物体检测相比，我们省略了沿Z轴的位置和尺寸，因为在像自主驾驶这样的应用中，感兴趣的物体被限制在同一个接地面上，因此我们只关心如何在那个平面上定位它。给定这样的参数化，回归分支的表示是{cos(θ)，sin(θ)，dx，dy，w，l}，对于位置(px，py)(如图3中的红点所示)。注意，航向角被分解为两个相关值，以加强角度范围约束。</p>
<p><img src="http://ww1.sinaimg.cn/large/006E0Yd9gy1gh8muzkdx8j30go0bs0th.jpg" alt="学习目标的定义"></p>
<p>在推理过程中，我们将θ解码为atan2(sin(θ)，cos(θ))。(dx，dy)对应于从像素位置到物体中心的位置偏移。(w，l)对应于对象大小。值得注意的是，对象位置和大小的值在现实世界的度量空间中。最后的学习目标是{cos(θ)，sin(θ)，dx，dy，log(w)，log(l)}，它在训练集之前被标准化为具有零均值和单位方差。</p>
<h3 id="Learning-and-Inference"><a href="#Learning-and-Inference" class="headerlink" title="Learning and Inference"></a>Learning and Inference</h3><p>与传统的多任务网络类似，其Loss的计算方式如下所示，其中focal_loss为分类的损失，smooth为回归的损失。<br><img src="http://ww1.sinaimg.cn/large/006E0Yd9gy1gh8muz8b1ej30fr06f74p.jpg" alt="损失函数"></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-01-14T16:00:00.000Z" title="1/15/2020, 12:00:00 AM">2020-01-15</time>发表</span><span class="level-item"><time dateTime="2020-03-19T02:20:38.000Z" title="3/19/2020, 10:20:38 AM">2020-03-19</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E6%9C%AF%E4%B8%9A/">术业</a></span><span class="level-item">35 分钟读完 (大约5194个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/01/15/Note_for_object_detection/">目标检测论文笔记</a></h1><div class="content"><h2 id="DPM模型：魔改版HOG-SVM"><a href="#DPM模型：魔改版HOG-SVM" class="headerlink" title="DPM模型：魔改版HOG+SVM"></a>DPM模型：魔改版HOG+SVM</h2><p>论文地址：A Discriminatively Trained, Multiscale, Deformable Part Model  </p>
<p>Dalal的论文结尾的未来展望部分曾提到，虽然采用固定的模板可以很好的解决的行人检测问题，但是采用可变的部件模型会具有更加普遍的应用场景。也就是本文的主要内容-DPM模型，其全称为可变部件模型。 该模型在首先由图像金字塔生成HOG特征金字塔，并在HOG特征金字塔上的上层采用一个固定的粗略的全局模板，在金字塔的底层采用精细的部件模板。通过这种方法来提高检测的精度。<br><img src="http://ww1.sinaimg.cn/large/006E0Yd9gy1gaz0jrccdrj30h5081aff.jpg" alt="DPM"></p>
<h3 id="本文主要贡献"><a href="#本文主要贡献" class="headerlink" title="本文主要贡献"></a>本文主要贡献</h3><ul>
<li>作者提出了一种多尺度的可变部件模型，用于解决通用类别的目标检测问题。</li>
<li>其检测精度比2006 PASCAL的冠军高了两倍，在2007 PASCAL挑战赛的20个类别中，在其中十个类别取得最佳结果。模型处理一张图片速度仅需两秒，现有模型中是最佳的</li>
<li>解决了在困难数据集上，deformable models通常表现不如conceptually weaker models的问题</li>
<li>提出了一种简单有效的策略，可以从弱标记数据中学习部件，其性能可以在单个CPU上三个小时内学习一个模型。</li>
<li>提出了一种新的判别培训的方法</li>
<li>引入了一种新的数据挖掘方法，用于在训练过程中挖掘“hard negative”的实例</li>
</ul>
<h3 id="系统概述"><a href="#系统概述" class="headerlink" title="系统概述"></a>系统概述</h3><ul>
<li>an object model &#x3D; 一个全局的 root filter + 几个 part models</li>
<li>Part model &#x3D; a spatial model + a part filter</li>
<li>空间模型(spatial model)定义了一个部件(part)的一组相对于检测窗口的可允许放置位置</li>
<li>检测窗口的score &#x3D; score of root filter + 部件上的总和</li>
</ul>
<h3 id="模型概述"><a href="#模型概述" class="headerlink" title="模型概述"></a>模型概述</h3><h2 id="R-CNN-Region-with-CNN-features"><a href="#R-CNN-Region-with-CNN-features" class="headerlink" title="R-CNN (Region with CNN features)"></a>R-CNN (Region with CNN features)</h2><p>Overfeat  &#x3D; sliding window + CNN architecture<br>RCNN      &#x3D; selective search + CNN architecture</p>
<h3 id="本文主要贡献-1"><a href="#本文主要贡献-1" class="headerlink" title="本文主要贡献"></a>本文主要贡献</h3><ul>
<li>一种简单可扩展的检测算法，比VOC 2012最佳结果(<em>DPM HSC</em>)的mAP值高30%（53.3%) 并且RCNN(mAP: 31.4%)在ILSVRC2013数据集的200个类别上性能优于OverFeat(mAP 24.3%)，</li>
<li>原理上的贡献：针对有标签的训练数据稀少的问题，作者发现，在大型辅助数据集上进行有监督的预训练，然后在小型数据集上进行domain-specific的微调是一种很有效的解决办法。</li>
</ul>
<h3 id="关键点"><a href="#关键点" class="headerlink" title="关键点"></a>关键点</h3><ul>
<li>将高容量的卷积神经网络(CNN)应用于自下而上的region proposal，用来对目标进行定位和分割</li>
<li>当带标签的训练数据不足，则采用有监督的pre-training.</li>
</ul>
<h3 id="模型概述-1"><a href="#模型概述-1" class="headerlink" title="模型概述"></a>模型概述</h3><p>R-CNN的目标检测系统由三个模块组成：</p>
<ol>
<li>生成与类别无关的region proposal</li>
<li>一个很大的卷积神经网络，用于从每个区域中提取固定长度的特征向量</li>
<li>一组基于特定类别的线性SVM分类器</li>
</ol>
<h4 id="Region-proposal"><a href="#Region-proposal" class="headerlink" title="Region proposal"></a>Region proposal</h4><p>R-CNN采用selective search为每个图像生成约2k个region proposal。 该方法采用bottom-up grouping和saliency cues的方法来提供准确的任意大小的候选框。</p>
<h3 id="feature-extraction"><a href="#feature-extraction" class="headerlink" title="feature extraction"></a>feature extraction</h3><p>在进行特征提取之前，需要对图像进行预处理操作，即将输入图像进行归一化操作，使其大小统一为227*227</p>
<h3 id="classification-and-localization"><a href="#classification-and-localization" class="headerlink" title="classification and localization"></a>classification and localization</h3><p>在分类上，RCNN对每个类别采用训练后的线性SVM分类器进行分类任务。SVM的训练过程分为两步，首先针对特定的类别对线性SVM进行预训练，训练集采用ILSVRC2012。然后采用特定于区域的微调。然后采用该分类器在正区域（目标）和负区域（背景）上进行打分，然后在被打分的区域上进行bounding box regression并使用NMS（greedy non-maximum suppression）进行过滤，以生成保留对象位置的最终边界框。对于标签不足的数据，R-CNN方法中并没有使用无监督的预训练，而是对非常大的辅助数据集ILSVRC进行有监督的预训练，然后进行特定于区域的微调。</p>
<h2 id="SPPnet"><a href="#SPPnet" class="headerlink" title="SPPnet"></a><a target="_blank" rel="noopener" href="https://blog.csdn.net/alibabazhouyu/article/details/80058009">SPPnet</a></h2><p>RCNN虽然在目标检测领域取得了非常优秀结果，但是仍然存在两个问题，首先图像在训练之前必须经过预处理，这会导致图像失真，严重影响检测的精度。其次图像需要对2k个预选框进行卷积运算，这带来了很大的计算负担，对检测的速度也造成很大影响。针对上述的两个问题，何凯明等人提出了SPPnet, 将图像金字塔应用于深度网络，提高了检测的速度和精度。</p>
<p>尽管RCNN在目标检测上取得的成绩是显著的，它仍然存在两个明显的缺点: </p>
<ol>
<li>在图像被输入到CNN结构中之前，需要对图像进行扭曲变换使图像统一为固定的大小。这两种操作将会导致内容的丢失和失真从而使检测的精度受到影响。</li>
<li>R-CNN采用Selective Search方法从一张图片中生成了约2k个候选区域，在特征提取阶段，CNN网络分别对2k个候选区域做卷积运算，这意味着也就是一张图片需要经过2000次CNN的前向传播，这个过程会产生大量的计算冗余，降低了检测的速度。</li>
</ol>
<p>**Spatial pyramid pooling: ** 在深度卷积神经网络结构中，卷积层可以处理任意大小的图像，因此对输入图像固定大小的约束仅来自于全连接层。其原因是全连接层的输入必须为固定长度的向量。为了解决这个问题，何等人在CNN的最后一个卷积层和全连接层之的池化层替换为空间金字塔池(如图6)。 空间金字塔池基于bag-of-word【引用】方法，他通过不同的size和stride但数量固定的local spatial bins来提取空间信息。因此无论输入图像的大小，空间金字塔池的输出仅和local spatial bins的数量有关。通过这种方法，不仅可以提高检测的鲁棒性，还可以保证输入图像的尺寸的灵活性，进而也可以改善网络的过拟合问题。</p>
<p>**Feature map computation: ** 针对RCNN网络的第二个缺陷，SPPnet仅从整个输入图像上计算一次特征图，然后在特征图的候选窗口上应用空间金字塔池化。在这里涉及到了一个问题：如何将窗口映射到特征图上？在这篇论文中，作者将窗口的角点映射到特征图上的一个像素上，是图像域中的的这个角点最接近该像素的感受域的中心。这种改进使得SPPnet在Pascal 2007上的检测速度比R-CNN方法快了24-102倍。</p>
<h2 id="Fast-RCNN"><a href="#Fast-RCNN" class="headerlink" title="Fast RCNN"></a><a target="_blank" rel="noopener" href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/">Fast RCNN</a></h2><p>2015年，Girshick et al. 在RCNN和SPPnet的基础上提出了Fast RCNN模型。在这篇论文中，Girshick et al. 系统性的总结了这两种网络的缺点并做出了相应的改进，使得模型的检测速度显著提升，同时也改善了检测的精度。在结果上，Fast RCNN在训练阶段比RCNN快九倍，比SPPnet快3倍。在测试时，检测网络比RCNN快了213倍，比SPPnet快10倍。在实际运行中，检测网络可以可以在0.3秒内处理图像(不包括区域建议时间)，同时在PASCAL VOC 2012上实现了66% mAP。 </p>
<p>**ROI Pooling layer: ** 在SPPnet中，何恺明等人采用空间金字塔池化来解决全连接层对图像大小约束的问题。Fast RCNN模型将该SPP层简化为单尺度的ROI(region of interest)层。 RoI层将候选区域划分为大小为H×W的网格，然后在每个网格上采用max-pooling，使得候选区的局部特征映射转变为大小统一的数据。Fast RCNN网络的具有两个数据输入：list of images and list of RoIs in those images 每个ROI具有两个输出： softmax概率和每个边界框的回归偏移。</p>
<p>**Single Stage Training: ** 在R-CNN和SPPnet中，训练的过程遵循多阶段的pipeline, 这主要包括：特征提取的训练, 并采用Log loss对网络进行微调，SVM分类器的训练和Bounding box regression的训练。这中多阶段的训练过程增加目标检测网络的复杂度并影响了检测速度。为了解决这个问题，Fast R-CNN网络采用了单阶段的训练方法。 首先网络采用装备了ROI层的VGG16网络初始化数据，然后采用反向传播对所有的网络权重进行训练。针对SPPnet的SPP层以下无法更新权重的问题，这篇论文采用了分层的采样方法，即针对每个SGD mini-batch 先对图像进行采样，然后对RoIs进行采样，并在同一图像的ROI中共享计算。除此之外，Fast R-CNN还在一次微调中联合优化softmax classifier和bounding box regression。这里Girshick et al.采用了多任务的损失 L：<br><img src="http://ww1.sinaimg.cn/large/006E0Yd9gy1gcrleg14yzj30eg01vglh.jpg"><br>这里L_cls是一个分支softmax层输出概率分布,L_loc是bounding box的损失函数。最终，Fast RCNN将模型整合成为一个整体的单阶段训练模型(除了region proposal), 通过这种方法提高了检测的速度和精度。</p>
<h2 id="Faster-RCNN"><a href="#Faster-RCNN" class="headerlink" title="Faster RCNN"></a><a target="_blank" rel="noopener" href="https://blog.csdn.net/u011534057/article/details/51259812">Faster RCNN</a></h2><p>Fast R-CNN通过共享卷积和单阶段的训练过程降低了目标检测的运算负担，同时也提高了检测效率和精确度。但是region proposal generation仍然是制约着Region based目标检测模型的一大瓶颈。2015年 Ren et al.在Fast R-CNN基础上提出了Faster R-CNN模型，Instead of Selective, Faster R-CNN通过训练一个全卷积网络来生成目标候选。 该网络被称为Region proposal Network(RPN)。Faster R-CNN将RPN与Fast R-CNN的检测网络整合起来应用于目标检测，最终该模型在Pascal VOC 2007的精确度达到 73.2% mAP(best result for state of the art)。 并且检测的帧率在GPU上达到了5fps。</p>
<p>*<em>Region proposal Network: **<br>为了得到目标提案的矩形框，RPN引入了全卷积网络。网络的输入是任意大小的图像，首先网络先计算出图像的卷积特征图。然后在最后一个卷积层上采用3</em>3的滑动窗口同时预测k个区域提案，在这里作者引入了具有平移不变性的锚点概念来描述区域提案，锚点位于滑动窗口的中心，在每个位置采用了3 scales and 3 aspect ratio，yielding k&#x3D;9的锚点。滑动窗口的输出被映射到一个低维的向量中，该向量将被馈送到回归层和分类层(见图6)。 reg layer得到区域提案的坐标，分类层通过对区域提案进行打分来评估区域中包含物体的概率。<br>为了对RPN网络进行训练，Ren et al.为anchor假设了一个二元的类标签：object&#x2F;not object。当锚点与ground truth的IoU(Intersection over region)高于0.7或者重叠，则该锚点将会得到一个positiv label(object)。训练所采用的损失函数与Fast R-CNN的相同。</p>
<p><strong>模型训练：</strong> RPN网络可以采用back-propagation和stochastic gradient descent进行端对端的训练。在faster R-CNN中，作者通过RPN和检测网络共享卷积层来加快模型的训练和检测速度，但这同时也产生了一个问题，整个faster R-CNN模型在共享卷积层的情况下由于RPN和检测网络的独立训练无法收敛。 在这里作者描述了一种交替优化的四步算法来解决该问题。</p>
<ol>
<li>首先，算法采用预训练的ImageNet模型对RPN进行初始化，并针对区域建议任务进行端对端的训练。</li>
<li>然后该算法采用RPN生成object proposals, 然后采用fast R-CNN的方法对检测网络进行训练，检测网络也采用ImageNet-pre-trained model进行初始化。</li>
<li>采用第二步训练的检测网络初始化RPN并对RPN进行训练，在此阶段两个网络开始共享卷积层，训练过程仅微调RPN特有的层。</li>
<li>保持共享卷积层不变，再次训练检测网络，训练过程仅微调Fast R-CNN特有的层。</li>
</ol>
<h2 id="YOLO-v1"><a href="#YOLO-v1" class="headerlink" title="YOLO v1"></a>YOLO v1</h2><p>2016年，Redmon等人提出的YOLO模型也是目标检测领域的里程碑之一。不同于R-CNN，fast R-CNN，faster R-CNN等两阶段模型，YOLO提出了一种unified architecture用于目标检测。该方法将目标检测的框架视为bounding boxes 和 class probabilities的回归问题，采用单个网络来解决。在检测速度上YOLO展现出了卓越的性能。基础YOLO模型以45帧&#x2F;秒的速度实时处理图像，作者还采用了一种更加小型的YOLO版本实现了150帧每秒的检测。<br><strong>detection pipeline:</strong> YOLO在整副图像上预测所有类别的边界框，首先，输入图像被调整大小为448*448的图像，然后卷积检测网络接收归一化的图像作为输入并输出通过模型的置信度进行打分的边界框。在具体细节上，输入图像首先被划分为 S x S的网格。如果目标的中心点位于某个网格中，则该网格负责目标的边界框的检测。每个网格负责预测B个预测边界框(x, y, w, h)和相应的置信度分数 ####. 除此之外，每个网格还要预测C个条件类别的概率： #### 最终每个图像的预测输出可以参数化为一个S x S x (B x 5 + C)的tensor (见下图).<br><img src="http://ww1.sinaimg.cn/large/006E0Yd9gy1gctx2h9rrij30hi0bvwkm.jpg" alt="Model pipeline"></p>
<p>**Network training: ** YOLO所采用的检测网络由24个卷积层和两个全连接层组成，为了提高检测的精度，网络的卷积层采用ImageNet 1000-class competition dataset进行预训练。模型的最后一层输出目标的bounding box和class probabilities。在模型激活函数上模型的最后一层采用了线性的激活函数，其它层采用了leaky rectified linear activation。在优化问题上，YOLO采用了sum-squared error优化模型的输出，正如下面的激活函数所展示的：</p>
<p>在这里l_{i}^{obj}代表网络i中存在对象，l_{ij}^{obj}代表网络i的第j个bounding box中存在对象.</p>
<p>模型详细解释：<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/cad68ca85e27">yolo v1详解</a></p>
<h2 id="YOLO-v2"><a href="#YOLO-v2" class="headerlink" title="YOLO v2"></a>YOLO v2</h2><p>尽管Redmon等人提出的YOLO在目标检测的速度上取得很大的突破，但该网络仍然存在诸多限制。</p>
<ol>
<li>由于边界框预测的空间约束，每个网格单元只预测两个含有有单个类别的bounding box，这在提高了检测速度的同时也降低了对多个成组出现的小物体的检测能力。</li>
<li>模型的检测精度不够高，略低于Faster R-CNN(见表格)</li>
<li>检测网络中的多个下采样层导致了图像信息的丢失，所提取的模型特征不够精细。<br>针对以上问题，YOLOv2相对v1版本，采用了多种方法在预测精度，检测速度和识别对象种类三个方面进行了改进。除此之外，作者还提出了一种多尺度的训练方法，使模型可以在速度和准确性之间做出权衡。结果上，YOLO V2以67FPS的速度运行时，在VOC 2007上的mAP值为76.8%，而以40FPS的速度运行时的mAP值为78.6%。由于YOLO v2模型能够检测9000种不同对象，因此该模型被称为YOLO9000。</li>
</ol>
<p><strong>Better:</strong> 针对模型的第一个缺点，Redmon et al.采用不同的方法来改进YOLO模型。首先作者在所有的卷积层上采用了batch normalization来解决反向传播过程中的梯度消失和梯度爆炸问题，并且可以对模型进行规范化，从而能够获得更好的收敛速度和收敛效果。其次，YOLO v2在分类器预训练后，再采用的高分辨率样本对分类器进行微调，这样可以避免缓解了分辨率突然切换造成的影响。然后，YOLO模型移除了全连接层并引入了anchor来预测边界框。通过该方法来提高模型的召回率。然后，在锚点框的使用上，YOLO v2在训练集的真实框上使用k-means clustering来得到先验框的尺度。但使用锚定框会遇到位置的预测不稳定的问题，这里Redmon et al. 通过添加约束的方法，将预测的中心限制在特定网格单元中。对于YOLO v1模型所提取的特征过于粗糙的问题，YOLO v2引入了一个新的passthrough层检测细粒度特征。具体来说，passthrough层通过将相邻要素堆叠到不同的通道中过来将高分辨率的特征和低分辨率的特征连接起来。使图像的细节信息尽可能的被保留下来。在模型的训练上，由于YOLO网络上去掉了全连接层，因此网络可以处理任意大小的图像。为了保证图像对不同尺度的图像的检测精度，YOLO v2在训练过程中采用不同尺度的图像进行训练，网络的每10个批次将会随机选择新的图像尺寸。最终使网络能够适应各种大小的目标的检测。</p>
<p>**Faster: **为了进一步提升速度，YOLO2提出了Darknet-19（有19个卷积层和5个MaxPooling层）网络结构。DarkNet-19比VGG-16小一些，精度不弱于VGG-16，但浮点运算量减少到约1&#x2F;5，以保证更快的运算速度。</p>
<p>**Stronger: **</p>
<h2 id="one-stage-model-vs-two-stage-model"><a href="#one-stage-model-vs-two-stage-model" class="headerlink" title="one stage model vs. two stage model"></a>one stage model vs. two stage model</h2><p>如下图所示，当我们对检测的管线进行横向比较时，可以很清楚的看到技术的发展趋势。在传统目标检测阶段，检测的pipeline分为三步：区域选择，特征提取和分类。在此阶段检测算法的特征是基于规则手工设计的。在基于深度学习的目标检测阶段，两阶段模型仍遵循传统的目标检测方法的思路，将深度学习技术整合入目标检测的pipeline中。正如图三所示，在R-CNN，SPPnet和Fast R-CNN中，检测模型引入卷积神经网络来提取图像特征，在Faster R-CNN中，卷积神经网络被同时应用于候选区域生成和检测阶段，通过共享卷积使整个检测模型被统一整合为完整的深度学习模型。而基于回归的单阶段检测模型则采用新的思路，直接采用深度学习技术从pixel直接预测物体，取消了区域生成的阶段，大大简化了模型的复杂程度。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-12-29T16:00:00.000Z" title="12/30/2019, 12:00:00 AM">2019-12-30</time>发表</span><span class="level-item"><time dateTime="2020-02-10T06:17:06.000Z" title="2/10/2020, 2:17:06 PM">2020-02-10</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E6%9C%AF%E4%B8%9A/">术业</a></span><span class="level-item">16 分钟读完 (大约2440个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/12/30/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%96%B9%E6%B3%95/">目标检测之方法论</a></h1><div class="content"><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>目标检测是近些年来计算机视觉和数字图像处理的一个研究热点，它是图像处理和计算机视觉学科的重要分支，广泛应用于自动驾驶，机器人导航、智能视频监控、工业检测、航空航天等诸多领域。它的主要任务是检测已知的类别在图像中的实例，它包含物体定位和物体分类两个子任务，同时确定物体的类别和位置。传统的的目标检测算法通过区域选择，特征提取，分类三个步骤实现对物体的检测。随着深度学习技术的发展，目标检测算法采用不同的深度学习网络来提高目标检测技术的速度和准确度。它可以分为 one stage, two stage 和 multi stage 三种类别。在这里本文主要回顾了目标检测的经典算法, 主要包括R-CNN, Fast R-CNN, Faster R-CNN。并采用已知的数据对这些算法的识别速度和检测精度进行了横向比较，并总结了不同算法的特点和应用范围。  </p>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>另一方面，由于卷积神经网络在目标检测领域的应用，使得基于深度学习的目标检测方法在近些年来发展十分迅速。2013年，Ross Girshick等人提出了R-CNN网络，首次CNN方法引入目标检测领域，大大提高了目标检测效果，同时改变了目标检测领域的主要研究思路。2015年，</p>
<h2 id="传统目标检测方法"><a href="#传统目标检测方法" class="headerlink" title="传统目标检测方法"></a>传统目标检测方法</h2><p><strong>区域选择：</strong><br><strong>特征提取：</strong><br><strong>分类：</strong>分类的目的是将图片用一种层次化的方法进行表述。在上一步中所提取的候选区域的特征将被送到分类器，分类器采用适当的算法对其进行分类，用事先确定好的类别和实例ID来描述图片。传统目标检测采用的分类器算法有：SVM，Adaboost，DPM等。</p>
<p>根据目标检测具体的应用场景，研究人员对以上三个步骤的具体算法做了各种各样的改进，以实现更高的准确率与更快检测速度。 在过去的20年里，研究人员在传统目标检测领域做了很多的研究，提出了一些可以看做里程碑式的经典算法，如图二所示，接下来我们将按照时间顺序介绍一些传统目标检测领域的重要算法。</p>
<h3 id="Haar-like特征-Adaboost"><a href="#Haar-like特征-Adaboost" class="headerlink" title="Haar-like特征 + Adaboost"></a>Haar-like特征 + Adaboost</h3><p>2001年，Viola和Jones提出了用于人脸识别的目标检测算法，这种算法大大提高了人脸检测的效率和精度。该算法主要基于三个重要步骤：Haar-like特征，Adaboost分类和Cascade级联分类器</p>
<p>论文采用了最简单的矩形特征作为Feature descripter, 作者创新性的提出了积分图的概念。该图像和原图像大小相同，其每个位置的值为像素点左上所有像素点之和。 采用这张积分图，使得计算任意矩形区域的特征值的运算可以通过矩形框四角的所对应的值相加减得到（见图2）</p>
<p>首先论文采用了哈尔矩形特征作为Feature descripter. 该特征通过计算矩形明暗区域的像素和差值来描述区域的特征。作者创新性的提出了积分图的方法来简化运算，该图像和原图像大小相同，其每个位置的值为像素点左上所有像素点之和。 采用该图可以使任意矩形区域的特征值通过积分图上矩形四个顶点的值进行运算，这大大降低了计算负担。<br>其次，</p>
<h3 id="HoG-SVM"><a href="#HoG-SVM" class="headerlink" title="HoG + SVM"></a>HoG + SVM</h3><p>HOG descripter是传统目标检测领域的重要的里程碑，它在SIFT，和形状上下文的基础上，在均匀建个的单元格密集网络上使用</p>
<p><strong>特征提取链：</strong> 输入图像首先被分成许多小的空间区域（6<em>16像素块，四个 8</em>8 的像素单元），通过收集像素中每个像素的梯度和边缘方向 可以计算一维面向梯度的直方图。 但是由于局部照明的差异以及前景和背景的对比度，梯度强度变化的范围非常大，因此需要对梯度的局部对比度进行归一化。 需要进行对比度归一化的步骤，以使描述符对光照，阴影和边缘变化具有鲁棒性。 然后，将规范化描述符（HOG描述符）填充到密集网格中的检测窗口中，并将由HOG描述符形成的特征向量放入线性SVM分类器中以实现二进制分类。 最后，检测窗口扫描输入图像的多尺度图像金字塔，通过非最大抑制来检测物体。在分类器的训练过程中，dalal等人采用一个两步的训练方法，使得误检率减少了5% 。</p>
<h3 id="DPM-LSVM"><a href="#DPM-LSVM" class="headerlink" title="DPM + LSVM"></a>DPM + LSVM</h3><p>值得一提的是，Dalal的论文所提出的HOG+SVM方法采用固定的模板来提取HOG特征，这种方法对于无法处理目标的形变问题。 2008年，P. Felzenswalb在HOG特征的基础上提出了DPM+LSVM的方法，这种方法在由HOG特征生成的金字塔上，采用一个粗略的模板来涵盖物体，和一个精细的模板来检测物体的可形变部分。 通过这种方法，作者在2007 PASCAL挑战赛的20个类别中的十个类别获得最佳结果。并且检测精度是2006 PASCAL冠军的两倍。作者也因此在2010年获得 PASCAL VOC的终身成就奖</p>
<p>**系统概述: **基于滑动窗口法，该目标检测系统由一个root filter和多个part models组成.</p>
<h2 id="基于深度学习的目标检测方法"><a href="#基于深度学习的目标检测方法" class="headerlink" title="基于深度学习的目标检测方法"></a>基于深度学习的目标检测方法</h2><p>1998年，Yann LeCun提出了LeNet-5的网络结构，该结构可以端到端训练模型以进行文档检测。 它成为最经典的CNN结构，以后的大部分CNN模型都源自此版本。 2012年，Kriszhevsky等人在物体检测领域采用了基于CNN的方法。 在2014年，Ross Girshick等人。 提出的R-CNN，其出色的性能吸引了研究人员的注意。 从那时起，深度学习成为对象检测领域的一个热点。</p>
<p>与具有手工功能的传统方法相比，基于CNN的方法可以更分层和更深入地表示特征。 典型的CNN结构由具有特征图的许多层组成。 特征图中的像素（神经元）通过权重连接到先前的特征图。 可以通过具有共享权重的卷积核从上一层的局部区域中获取每一层的特征。 输入图像将在网络中重复计算，低级特征将在初始层中计算，而高级特征则可在后续层中提取。 结果上，模型的表达能力将通过CNN结构得到了指数级的提高。 特征可以以较高的语义层面来表达。</p>
<p>随着研究的不断深入，基于深度学习的方法发展出了两种不同的方向。 第一种是以RCNN, Fast RCNN，Faster RCNN为代表的两阶段模型(two stage frameworks)，又称作基于区域模型(region based frameworks)。该方法遵循传统的目标检测流程，首先生成候选区域(region proposal)，然后根据所提取的特征将这些候选区域分为不同的类别。 第二种是基于分类&#x2F;回归的一步框架，它可以直接从图像的像素映射到边界框坐标和类别的概率。通过这种方法可以大大降低时间开销。其代表模型有： Yolo, SSD。</p>
<h3 id="One-stage-Model"><a href="#One-stage-Model" class="headerlink" title="One stage Model"></a>One stage Model</h3><p>随着上一小节提到的RCNN，Fast R-CNN, Faster R-CNN等论文对两阶段模型的潜力的发掘，两阶段模型的检测精度不断的提高。 但同时由于两阶段模型需要先生成区域候选的步骤，其检测速度成为了限制其性能的瓶颈，并且由于其计算成本的问题，使其也难以应用到性能比较中庸的穿戴设备或移动设备中去。 为了解决这一问题，研究人员提出了一种新的统一的检测架构，其不同于two Stage modeld的架构，该统一架构的不需要生成区域预选，直接从整幅图像上预测目标的边界框坐标和类别概率。通过这种方法大大提高了检测的速度。该架构被称作One Stage Model.</p>
<h3 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h3><p>在R-CNN出现之前，最成功的目标检测模型是DPM，其在VOC数据集上的最佳表现是mAP 34.3%左右，但是Ross Girshick在2013年提出的R-CNN方法将这个</p>
</div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="Your name"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Your name</p><p class="is-size-6 is-block">Your title</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Your location</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">32</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">3</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">15</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/ppoffice" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/ppoffice"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/%E6%85%8E%E6%80%9D/"><span class="level-start"><span class="level-item">慎思</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%9C%AF%E4%B8%9A/"><span class="level-start"><span class="level-item">术业</span></span><span class="level-end"><span class="level-item tag">27</span></span></a></li><li><a class="level is-mobile" href="/categories/%E7%8E%A9%E7%89%A9/"><span class="level-start"><span class="level-item">玩物</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><figure class="media-left"><a class="image" href="/2022/03/25/Notebook/Hexo/IcarusThemeConfig/"><img src="http://ww1.sinaimg.cn/large/006E0Yd9gy1gay01tguhxj30go07s3yj.jpg" alt="Hexo icarus主题设置"></a></figure><div class="media-content"><p class="date"><time dateTime="2022-03-24T16:00:00.000Z">2022-03-25</time></p><p class="title"><a href="/2022/03/25/Notebook/Hexo/IcarusThemeConfig/">Hexo icarus主题设置</a></p><p class="categories"><a href="/categories/%E6%9C%AF%E4%B8%9A/">术业</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2022/03/04/Notebook/Linux/Linux_Bash/"><img src="http://ww1.sinaimg.cn/large/006E0Yd9gy1gay01tguhxj30go07s3yj.jpg" alt="Linux Bash速查"></a></figure><div class="media-content"><p class="date"><time dateTime="2022-03-03T16:00:00.000Z">2022-03-04</time></p><p class="title"><a href="/2022/03/04/Notebook/Linux/Linux_Bash/">Linux Bash速查</a></p><p class="categories"><a href="/categories/%E6%9C%AF%E4%B8%9A/">术业</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2021/05/09/work/005.FreeRTOS/"><img src="http://ww1.sinaimg.cn/large/006E0Yd9gy1gaeafbwql6j30sg0g0aa5.jpg" alt="FreeRTOS学习笔记02"></a></figure><div class="media-content"><p class="date"><time dateTime="2021-05-08T16:00:00.000Z">2021-05-09</time></p><p class="title"><a href="/2021/05/09/work/005.FreeRTOS/">FreeRTOS学习笔记02</a></p><p class="categories"><a href="/categories/%E6%9C%AF%E4%B8%9A/">术业</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2021/05/07/work/004.RTOS/"><img src="http://ww1.sinaimg.cn/large/006E0Yd9gy1gaeafbwql6j30sg0g0aa5.jpg" alt="FreeRTOS学习笔记01"></a></figure><div class="media-content"><p class="date"><time dateTime="2021-05-06T16:00:00.000Z">2021-05-07</time></p><p class="title"><a href="/2021/05/07/work/004.RTOS/">FreeRTOS学习笔记01</a></p><p class="categories"><a href="/categories/%E6%9C%AF%E4%B8%9A/">术业</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2021/05/03/work/Emmbed_system_QA/"><img src="http://ww1.sinaimg.cn/large/006E0Yd9gy1gaeafbwql6j30sg0g0aa5.jpg" alt="嵌入式面试常见问题整理"></a></figure><div class="media-content"><p class="date"><time dateTime="2021-05-02T16:00:00.000Z">2021-05-03</time></p><p class="title"><a href="/2021/05/03/work/Emmbed_system_QA/">嵌入式面试常见问题整理</a></p><p class="categories"><a href="/categories/%E6%9C%AF%E4%B8%9A/">术业</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2022/03/"><span class="level-start"><span class="level-item">三月 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/05/"><span class="level-start"><span class="level-item">五月 2021</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/02/"><span class="level-start"><span class="level-item">二月 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/01/"><span class="level-start"><span class="level-item">一月 2021</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/05/"><span class="level-start"><span class="level-item">五月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/04/"><span class="level-start"><span class="level-item">四月 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/03/"><span class="level-start"><span class="level-item">三月 2020</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/02/"><span class="level-start"><span class="level-item">二月 2020</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/01/"><span class="level-start"><span class="level-item">一月 2020</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/12/"><span class="level-start"><span class="level-item">十二月 2019</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/05/"><span class="level-start"><span class="level-item">五月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/03/"><span class="level-start"><span class="level-item">三月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/02/"><span class="level-start"><span class="level-item">二月 2019</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Apollo/"><span class="tag">Apollo</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Blogs/"><span class="tag">Blogs</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/C/"><span class="tag">C++</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CV/"><span class="tag">CV</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux/"><span class="tag">Linux</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Self-driving/"><span class="tag">Self-driving</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Tensorflow/"><span class="tag">Tensorflow</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/nvidia/"><span class="tag">nvidia</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/object-detection/"><span class="tag">object detection</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/photo/"><span class="tag">photo</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/point-cloud/"><span class="tag">point cloud</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/pytorch/"><span class="tag">pytorch</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%B5%8C%E5%85%A5%E5%BC%8F/"><span class="tag">嵌入式</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%BE%B7%E5%9B%BD%E9%A9%BE%E7%85%A7/"><span class="tag">德国驾照</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%88%AA%E6%8B%8D/"><span class="tag">航拍</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">订阅更新</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="订阅"></div></div></form></div></div></div><div class="card widget"><div class="card-content"><div class="notification is-danger">You need to set <code>client_id</code> and <code>slot_id</code> to show this AD unit. Please set it in <code>_config.yml</code>.</div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="订阅"></div></div></form></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="Hexo" height="28"></a><p class="is-size-7"><span>&copy; 2022 John Doe</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">共<span id="busuanzi_value_site_uv">0</span>个访客</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>